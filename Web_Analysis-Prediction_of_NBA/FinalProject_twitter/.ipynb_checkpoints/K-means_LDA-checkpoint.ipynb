{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- programming start---\n",
      "Get result from /Users/cosoet/SIT/BIA660-WebAnalytics/FinalProject/GameResult/Thunder.csv\n",
      "--- programming end---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sample_package.commonFunction as basefunc\n",
    "import sample_package.filterMethod as filter_TFID\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from numpy  import array\n",
    "import numpy as np\n",
    "from scipy.misc import toimage\n",
    "\n",
    "\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "foundCnt = 0;\n",
    "foundMax = 1000;\n",
    "tweetIn=[];\n",
    "labelIn=[];\n",
    "nrow='';\n",
    "text = []\n",
    "target = []\n",
    "\n",
    "# get Game Infomation\n",
    "\n",
    "    \n",
    "print(\"--- programming start---\")\n",
    "#let's try team \"Thunder\" far example\n",
    "# if you want to try multiple teams, append team names into list.\n",
    "teamNameList = [ \"Thunder\"] # , \"Blazers\",\"Grizzlies\"\n",
    "\n",
    "for teamName in teamNameList:\n",
    "\n",
    "    gameInfo = basefunc.getGameInfo(teamName);\n",
    "\n",
    "    for eachGameInfo in gameInfo:\n",
    "\n",
    "        # Game data and time\n",
    "        gameDate = eachGameInfo.Date\n",
    "        gameTime = eachGameInfo.Time\n",
    "\n",
    "\n",
    "#         print(\"Team: {} Date: {} {}\".format(teamName, gameDate, eachGameInfo.result))\n",
    "        fileName = basefunc.getCvsPathByGameData(teamName, gameDate)\n",
    "        foundCnt+=1\n",
    "        if (len(fileName) == 0):\n",
    "            continue;\n",
    "        \n",
    "        fileName = ''.join(fileName)\n",
    "        nrow='';\n",
    "        file=open(fileName, 'r', encoding='utf-8')\n",
    "        csvCursor=csv.reader(file)\n",
    "        \n",
    "        for idx, row in enumerate(csvCursor):\n",
    "            if idx>0:\n",
    "        #For here,if you want to run with sentiment, please comment block 1. and open block 2.\n",
    "\n",
    "                #-------------block 1. without sentiment-------------#\n",
    "                nrow+=row[0]\n",
    "                #------------- end of block 1. -----------#\n",
    "\n",
    "\n",
    "                #-------------block 2. sentiment-------------\n",
    "    #             tpt = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', row[0], flags=re.MULTILINE)\n",
    "    #             tpresult = sid.polarity_scores(tpt)\n",
    "    #             if(tpresult[\"compound\"] != 0):\n",
    "    #                 tpt = tpt.replace(teamName, '')\n",
    "    #                 nrow += tpt\n",
    "            #------------- end of block 2. ------------#\n",
    "        tweetIn.append(nrow)\n",
    "        \n",
    "        if(eachGameInfo.result=='W'):        \n",
    "            labelIn.append(eachGameInfo.result)\n",
    "        elif(eachGameInfo.result=='L'):\n",
    "            labelIn.append(eachGameInfo.result)\n",
    "\n",
    "        \n",
    "\n",
    "    if foundCnt >= foundMax:\n",
    "        break;\n",
    "print(\"--- programming end---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words=\"english\",min_df=5) \n",
    "\n",
    "# generate tfidf matrix\n",
    "dtm= tfidf_vect.fit_transform(tweetIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cosoet/anaconda3/lib/python3.6/site-packages/nltk/cluster/util.py:133: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    }
   ],
   "source": [
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "\n",
    "# set number of clusters\n",
    "num_clusters=2\n",
    "\n",
    "# initialize clustering model\n",
    "# using cosine distance\n",
    "# clustering will repeat 10 times\n",
    "# each with different initial centroids\n",
    "clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=10)\n",
    "\n",
    "# samples are assigned to cluster labels starting from 0\n",
    "clusters = clusterer.cluster(dtm.toarray(), assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.14561064e-03 7.48827478e-04 1.97060178e-04 ... 9.06939656e-05\n",
      "  0.00000000e+00 1.82305852e-04]\n",
      " [8.34937817e-03 1.41785143e-03 2.72509832e-04 ... 1.68857483e-04\n",
      "  1.91692526e-04 2.93167175e-04]]\n",
      "Cluster 0: thunder; com; http; maps; twitter; https; google; pic; shock; vs; westbrook; www; rain; nba; mareep; 15; game; youtu; city; oklahoma \n",
      "Cluster 1: thunder; com; http; twitter; https; pic; rain; www; vs; nba; youtu; city; oklahoma; game; youtube; voice; words; flowers; maps; grows \n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# clusterer.means() contains the centroids\n",
    "# each row is a cluster, and \n",
    "# each column is a feature (word)\n",
    "centroids=np.array(clusterer.means())\n",
    "print(centroids)\n",
    "\n",
    "sorted_centroids = centroids.argsort()[:, ::-1] \n",
    "\n",
    "voc_lookup= tfidf_vect.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # get words with top 20 tf-idf weight in the centroid\n",
    "    top_words=[voc_lookup[word_index] \\\n",
    "               for word_index in sorted_centroids[i, :20]]\n",
    "    print(\"Cluster %d: %s \" % (i, \"; \".join(top_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual_class</th>\n",
       "      <th>L</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual_class   L   W\n",
       "cluster             \n",
       "0             11  19\n",
       "1             23  28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(list(zip(labelIn, clusters)), \\\n",
    "                columns=['actual_class','cluster'])\n",
    "df.head()\n",
    "pd.crosstab( index=df.cluster, columns=df.actual_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: K-Mean clustering result /n\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.45      0.68      0.54        34\n",
      "        Win       0.63      0.40      0.49        47\n",
      "\n",
      "avg / total       0.56      0.52      0.51        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#---test K-Mean clustering---#\n",
    "\n",
    "cluster_dict={0:'Win', 1:\"Lose\"}\n",
    "meaningful_dict={'W':'Win', 'L':\"Lose\"}\n",
    "# Assign true class to cluster\n",
    "predicted_target=[cluster_dict[i] for i in clusters]\n",
    "meaningful_target=[meaningful_dict[i] for i in labelIn]\n",
    "\n",
    "\n",
    "print(\"Task1: K-Mean clustering result /n\")\n",
    "print(metrics.classification_report(meaningful_target, predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LDA can only use raw term counts for LDA \n",
    "tf_vectorizer = CountVectorizer(max_df=0.75, min_df=5, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(tweetIn)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "X_train, X_test = train_test_split(tf, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cosoet/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 25, perplexity: 9572.8332\n",
      "iteration: 2 of max_iter: 25, perplexity: 8721.5110\n",
      "iteration: 3 of max_iter: 25, perplexity: 8437.7953\n",
      "iteration: 4 of max_iter: 25, perplexity: 8308.2020\n",
      "iteration: 5 of max_iter: 25, perplexity: 8237.0944\n",
      "iteration: 6 of max_iter: 25, perplexity: 8193.0440\n",
      "iteration: 7 of max_iter: 25, perplexity: 8163.1910\n",
      "iteration: 8 of max_iter: 25, perplexity: 8141.4693\n",
      "iteration: 9 of max_iter: 25, perplexity: 8124.7005\n",
      "iteration: 10 of max_iter: 25, perplexity: 8111.0869\n",
      "iteration: 11 of max_iter: 25, perplexity: 8099.5554\n",
      "iteration: 12 of max_iter: 25, perplexity: 8089.4436\n",
      "iteration: 13 of max_iter: 25, perplexity: 8080.3322\n",
      "iteration: 14 of max_iter: 25, perplexity: 8071.9585\n",
      "iteration: 15 of max_iter: 25, perplexity: 8064.1621\n",
      "iteration: 16 of max_iter: 25, perplexity: 8056.8447\n"
     ]
    }
   ],
   "source": [
    "#-- Test LDA model --\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 2\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                max_iter=25,verbose=1,\n",
    "                                evaluate_every=1, n_jobs=1,\n",
    "                                random_state=0).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task2: LDA with single-label \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Lose       0.36      0.38      0.37        34\n",
      "        Win       0.53      0.51      0.52        47\n",
      "\n",
      "avg / total       0.46      0.46      0.46        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X_train, X_test\n",
    "# Generate topic assignment of each document\n",
    "topic_assign=lda.transform(tf)\n",
    "\n",
    "#tranform to index\n",
    "maxIdxLst = np.argmax(topic_assign, axis=1)\n",
    "\n",
    "cluster_dict={0:'Win', 1:\"Lose\"}\n",
    "meaningful_dict={'W':'Win', 'L':\"Lose\"}\n",
    "\n",
    "# Assign true class to cluster\n",
    "predicted_target=[cluster_dict[i] for i in maxIdxLst]\n",
    "meaningful_target=[meaningful_dict[i] for i in labelIn]\n",
    "\n",
    "print(\"Task2: LDA with single-label \\n\")\n",
    "print(metrics.classification_report(meaningful_target, predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
